name: competitive-intel
version: "1.2"
schema_version: 2
description: >
  Structured competitive intelligence workflow for scoping the subject product,
  identifying competitors, collecting evidence-backed profiles, building
  comparison matrices, defining response playbooks, and setting monitoring.
author: "Loom Team"
tags: [competitive, intelligence, research, comparison, battlecards]

persona: |
  You are a competitive intelligence analyst who focuses on actionable,
  evidence-backed differences. You care about what competitors actually do,
  not what they claim. Use pricing pages, changelogs, hiring signals,
  customer reviews, and partnership signals as evidence.

phase_mode: strict

tool_guidance: |
  Favor structured tables over long narrative. Use CSV for matrices and
  Markdown for concise synthesis. Mark uncertain data points with a leading
  tilde (~), including source and recency marker.

tools:
  guidance: >
    Use web_search/web_fetch for competitive signals and document_write for
    battlecard-ready outputs.
  required: [web_search, web_fetch, read_file, write_file, document_write, search_files]
  excluded: [delete_file, shell_execute, git_command]

phases:
  - id: scope-subject
    description: >
      Define the subject offering, target buyer, primary use cases, and buying
      criteria before competitor comparison.
    depends_on: []
    model_tier: 2
    verification_tier: 1
    is_critical_path: true
    acceptance_criteria: >
      Subject company baseline includes product scope, ICP, core workflows,
      pricing posture, and top buying criteria for comparison.
    deliverables:
      - "subject-baseline.md — subject product scope and ICP baseline"
      - "buying-criteria.csv — weighted buying criteria for comparison"

  - id: identify-competitors
    description: >
      Build and prioritize competitor list including direct, indirect, and
      emerging threats.
    depends_on: [scope-subject]
    model_tier: 1
    verification_tier: 1
    is_critical_path: true
    acceptance_criteria: >
      At least 5 competitors with website, one-line description, competitor
      type, estimated relevance, and threat rationale.
    deliverables:
      - "competitor-list.csv — identified competitors with categorization"

  - id: research-competitors
    description: >
      For high-relevance competitors, gather evidence on capabilities, pricing,
      target customer, GTM strategy, momentum signals, and recent moves.
    depends_on: [identify-competitors]
    model_tier: 2
    verification_tier: 1
    is_critical_path: true
    acceptance_criteria: >
      Each high-relevance competitor has at least 5 of 7 dimensions covered,
      with source links and recency labels.
    deliverables:
      - "competitor-profiles.md — detailed profiles of top competitors"
      - "signal-log.csv — observed signals with date and source"

  - id: build-comparison
    description: >
      Create structured comparison matrix against weighted buying criteria and
      identify where the subject is stronger, weaker, or at parity.
    depends_on: [research-competitors]
    model_tier: 2
    verification_tier: 2
    is_critical_path: true
    acceptance_criteria: >
      Matrix includes at least 8 dimensions and win/lose/parity calls per
      competitor with evidence references.
    deliverables:
      - "comparison-matrix.csv — feature and capability comparison grid"
      - "competitive-summary.md — advantages, vulnerabilities, and implications"

  - id: response-playbook
    description: >
      Convert insights into response plan for product, sales, and marketing.
    depends_on: [build-comparison]
    model_tier: 2
    verification_tier: 2
    is_critical_path: true
    acceptance_criteria: >
      Includes top 3 strategic responses, owner role, timing, expected impact,
      and risks. Sales battlecards are concrete and competitor-specific.
    deliverables:
      - "battlecards.md — competitor-specific sales and positioning battlecards"
      - "response-backlog.csv — prioritized response actions with owners"

  - id: monitoring-plan
    description: >
      Define ongoing monitoring watchlist with signal thresholds that trigger
      response updates.
    depends_on: [response-playbook]
    model_tier: 2
    verification_tier: 2
    is_critical_path: true
    is_synthesis: true
    acceptance_criteria: >
      Watchlist includes signal type, source, recency expectation, threshold,
      and owner for response updates.
    deliverables:
      - "watchlist.csv — signals to monitor with thresholds and owners"
      - "alert-thresholds.md — rules for when to update playbooks"

tests:
  - id: smoke
    mode: deterministic
    goal: "Run the competitive-intel process and produce all declared deliverables."
    timeout_seconds: 900
    acceptance:
      phases:
        must_include: [scope-subject, identify-competitors, research-competitors, build-comparison, response-playbook, monitoring-plan]
      deliverables:
        must_exist: [subject-baseline.md, buying-criteria.csv, competitor-list.csv, competitor-profiles.md, signal-log.csv, comparison-matrix.csv, competitive-summary.md, battlecards.md, response-backlog.csv, watchlist.csv, alert-thresholds.md]
      verification:
        forbidden_patterns: ["Expected deliverable '.*' not found", "\\[TBD\\]|\\[TODO\\]|\\[INSERT\\]|\\[PLACEHOLDER\\]"]

  - id: live-canary
    mode: live
    goal: "Perform competitive intelligence on a live market segment and synthesize recommendations."
    timeout_seconds: 1200
    requires_network: true

verification:
  policy:
    mode: llm_first
    static_checks:
      tool_success_policy: safety_integrity_only
      file_integrity_policy: enforce_nonempty_changed_files
      syntax_policy: enforce_parseable_artifacts
      deliverable_existence_policy: enforce_declared_deliverables
    semantic_checks:
      - id: process-rules
        check: "Apply process-defined verification rules and acceptance criteria."
        severity: error
        enforcement: hard
        scope: phase
    output_contract:
      required_fields:
        - passed
        - outcome
        - reason_code
        - severity_class
        - confidence
        - feedback
        - issues
        - metadata
      metadata_fields:
        - remediation_required
        - remediation_mode
        - missing_targets
    outcome_policy:
      pass_states: [pass, pass_with_warnings, partial_verified]
      fail_states: [fail]
  remediation:
    strategies:
      - id: targeted-remediation
        retry_strategy: unconfirmed_data
        description: "Perform targeted follow-up based on verifier reason_code/metadata."
    default_strategy: targeted-remediation
    critical_path_behavior: block
    retry_budget:
      max_attempts: 2
    transient_retry_policy:
      retry_on_transient: true
  rules:
    - name: evidence-backed
      description: "Competitive claims must reference observable evidence"
      check: >
        Every major claim should reference specific evidence (feature behavior,
        pricing datapoint, review signal, hiring signal, or comparable source).
      severity: error
      type: llm
      applies_to_phases: [research-competitors, build-comparison, response-playbook, monitoring-plan]

    - name: subject-alignment
      description: "Comparison must align to subject buying criteria"
      check: >
        Validate comparison dimensions and recommendations map back to declared
        subject baseline and buying criteria.
      severity: error
      type: llm
      applies_to_phases: [build-comparison, response-playbook]

    - name: freshness-stated
      description: "Time-sensitive datapoints must include recency"
      check: >
        Pricing, product capabilities, and strategic moves should include a
        recency marker where applicable.
      severity: warning
      type: llm
      applies_to_phases: [research-competitors, monitoring-plan]

    - name: recommendations-actionable
      description: "Response recommendations must be specific and assignable"
      check: >
        Validate recommendations include owner role, timing, expected outcome,
        and update triggers.
      severity: error
      type: llm
      applies_to_phases: [response-playbook, monitoring-plan]

    - name: no-placeholders
      description: "No placeholder text left in deliverables"
      check: "\\[TBD\\]|\\[TODO\\]|\\[INSERT\\]|\\[PLACEHOLDER\\]"
      severity: error
      type: regex
      applies_to_phases: ["*"]

evidence:
  record_schema:
    required_fields: [evidence_id, tool, source_url, created_at, quality]
    facets: [target, entity, subject, region, category, segment, topic]
  extraction:
    include_result_facets: true
    arg_mappings:
      target: [target, entity, subject, company, market]
      region: [region, geography, country]
      category: [category, dimension, theme]
  summarization:
    max_entries: 12
    include_facets: true

prompt_contracts:
  evidence_contract:
    enabled: true
    applies_to_phases: ["*"]
  executor_constraints: |
    Keep factual claims traceable to evidence IDs or source URLs.
    If a claim cannot be verified, label it explicitly as unverified.
  verifier_constraints: |
    Evaluate acceptance criteria and process verification rules without assuming
    a fixed output schema. Use metadata keys only when inferable.
  remediation_instructions:
    default: |
      Perform targeted remediation only for failing checks. Preserve validated
      work, add missing evidence links, and avoid broad reruns.

memory:
  extract_types:
    - type: competitor_profile
      description: "Key facts about competitor product, pricing, positioning, and moves"
    - type: competitive_gap
      description: "Dimension where subject is weaker or stronger"
    - type: response_action
      description: "Recommended product/GTM response with owner and timing"
  extraction_guidance: |
    Focus on evidence that changes product or sales strategy. Include
    competitor name, source, and recency marker.

workspace_analysis:
  scan_for:
    - "**/*.csv — competitor data, matrices, or battlecard trackers"
    - "**/*.md — prior competitive analyses, battle cards, or launch notes"
  guidance: |
    Start from existing artifacts where credible, then refresh stale points and
    fill gaps with current evidence.

planner_examples:
  - goal: "Build competitive comparison of project management tools"
    subtasks:
      - { id: scope-subject, description: "Define product scope, ICP, and buying criteria", depends_on: [], model_tier: 2 }
      - { id: identify-competitors, description: "List direct/indirect competitors and prioritize", depends_on: [scope-subject], model_tier: 1 }
      - { id: research-competitors, description: "Collect evidence-backed profiles", depends_on: [identify-competitors], model_tier: 2 }
      - { id: build-comparison, description: "Build weighted comparison matrix", depends_on: [research-competitors], model_tier: 2 }
      - { id: response-playbook, description: "Convert gaps into battlecards and response backlog", depends_on: [build-comparison], model_tier: 2 }
      - { id: monitoring-plan, description: "Define watchlist and alert thresholds", depends_on: [response-playbook], model_tier: 2 }

replanning:
  triggers: |
    - Landscape is too fragmented to compare meaningfully
    - A key competitor is acquired, merged, or exits during analysis
    - Subject product scope is unclear
  guidance: |
    If landscape is fragmented, narrow to top threats and document long-tail
    exclusions. If subject scope is unclear, refresh scope-subject first.
